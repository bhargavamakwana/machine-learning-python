# -*- coding: utf-8 -*-
"""ML_Assignment_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10WLVmT7CaJdbyrsCIxmIFzhx_optDnht

Introduction to Machine learning (Assignment 1)

# **Problem 1**
"""

!pip3 install torch torchvision torchaudio scipy==1.7.1

import torch
import scipy
import numpy as np
import scipy.io
import matplotlib.pyplot as plt
from numpy.linalg import inv, pinv
from sklearn.utils import shuffle
# scipy version should be 1.7.1 or higher
print(scipy.__version__)

# load the mat files
mat_dataset = scipy.io.loadmat('/content/drive/MyDrive/ML_Homeworks/problem1.mat')
dX = mat_dataset['x']
dY = mat_dataset['y']

def data_split(x, y, ratio = 0.5):
  """
  split your data into train and test set
  """
  xshape = x.shape[0]
  yshape = y.shape[0]
  xid = int(xshape * ratio)
  yid = int(yshape * ratio)
  return x[:xid], x[xid:], y[:yid], y[yid:]

dX_train, dX_test, dY_train, dY_test = data_split(dX, dY)

# custom function for hypothesis for an N-degree polynomial function model.
def genpolyinput(x, d = 1):
  """
  Generate N-degree polyomial dataset.
  """
  helper = x.copy()
  helper = helper[np.newaxis, ...]
  X = np.ones(x.shape, dtype=np.int64)
  X = X[np.newaxis, ...]
  for i in range(d):
    X = np.concatenate((X, helper), axis = 0)
    helper = helper*x
  X = np.concatenate((X), axis=1)
  return X


def findModel(X, y):
  """
  The model is computed on the train dataset so that it can be used for test
  purposes.
  """
  # Note that here pinv is taken. So was in MATLAB. It throws incorrect values
  # for simple inverse.
  model = np.dot(pinv(X), y)
  return model

def findHypothesis(x, model):
  """
  return the hypothesis of the givendataset using the model.
  """
  return np.dot(x, model)

# Find the Emperical risk on the dataset for a d-degree polynomial
train_loss = np.zeros((1,41))
test_loss = np.zeros((1,41))
dimensions = np.arange(41, dtype=np.int64)

for i in range(1,41):
  
  # generate train polynomial input and predict the output based on the model
  X_train = genpolyinput(dX_train, i)
  model = findModel(X_train, dY_train)
  pred = findHypothesis(X_train, model)
  
  # Find Mean squared emperical loss of the model on the dataset
  remp = (dY_train - pred)**2 / (2*dX_train.shape[0])
  train_loss[0,i] = remp.sum()

  # Use the model to find the test loss and the performance on the test dataset.
  # This step is called cross validation
  X_test = genpolyinput(dX_test, i)
  pred = findHypothesis(X_test, model)
  remp = (dY_test - pred)**2 / (2*dX_test.shape[0])
  test_loss[0,i] = remp.sum()
  
# Compare and plot the Train loss and Test loss with respect to dimensions.
# Here dimensions is out Hyper parameter.

plt.figure(figsize=(16, 6))
plt.plot(dimensions[:], train_loss[0,:], color = 'orange')
plt.plot(dimensions[:], test_loss[0,:], color = 'green')
plt.show()


plt.scatter(dX, dY, color='blue')
X_final = genpolyinput(dX, 5)
model = findModel(X_final, dY)
plt.scatter(dX, findHypothesis(X_final, model), color='black')

"""# **Problem 2**"""

mat_dataset = scipy.io.loadmat('/content/drive/MyDrive/ML_Homeworks/problem2.mat')
dX = mat_dataset['x']
dY = mat_dataset['y']
dX_train, dX_test, dY_train, dY_test = data_split(dX, dY, 0.5)
train_loss = []
test_loss = []
lembda = np.arange(0,1000, 0.5)
xtx = np.dot(dX_train.T, dX_train)
for l in np.arange(0,1000, 0.5):
  # generate train polynomial input and predict the output based on the model
  model = np.dot(np.dot(inv(xtx + (l*np.identity(xtx.shape[0]))), dX_train.T), dY_train)
  pred1 = findHypothesis(dX_train, model)

  # Find Mean squared emperical loss of the model on the dataset
  remp = (dY_train - pred1)**2  / (2*dX_train.shape[0])
  train_loss.append(remp.sum())
  pred2 = findHypothesis(dX_test, model)
  # Use the model to find the test loss and the performance on the test dataset.
  # This step is called cross validation
  remp = (dY_test - pred2)**2 / (2*dX_test.shape[0])
  test_loss.append(remp.sum())

plt.figure(figsize=(16, 6))
plt.plot(lembda[:], train_loss, color = 'orange')
plt.plot(lembda[:], test_loss, color = 'green') 
plt.show()

"""# **Problem 4**"""

mat_dataset = scipy.io.loadmat('/content/drive/MyDrive/ML_Homeworks/dataset4.mat')
dX = mat_dataset['X']
dY = mat_dataset['Y']


epsilon = 1e-3
alpha = 0.01

model = np.ones((3,1))
model_old = 10*np.ones((3,1))
count = 0
errs = []
risks = []
def sigmoid(x, model):
  """
  """
  return 1/(1 + np.exp(-(x@model)))


def logistic_gradient(f, x, y):
  """
  """
  return np.dot((f - y).T, x).T

while abs(model_old - model).sum() >= epsilon:
  
  pred = sigmoid(dX, model)
  #print(pred.shape, dY.shape)
  gradient = logistic_gradient(pred, dX, dY)
  risk = abs((np.multiply((dY - 1), np.log(1 - pred)) - np.multiply(dY.T, np.log(pred))) / dY.shape[0])
  risk[np.isnan(risk)] = 0
  pred[pred >= 0.5] =  1
  pred[pred < 0.5] =  0
  err = (pred - dY) / dY.shape[0]
  #r = (y-1).* log(1- f )-y .*log (f ); r(isnan (r )) = 0;
  #R = mean(r );
  #print(gradient.shape)
  errs.append(err.sum())
  risks.append(risk.mean())
  model_old = model.copy()
  model = model - alpha*(gradient)
  count += 1

pred = sigmoid(dX, model) 
pred[pred >= 0.5] =  1
pred[pred < 0.5] =  0
plt.figure(figsize=(16, 6))
print(count)

plot_pred = []

for i in np.arange(0, 1, 0.04):
  plot_pred.append( (- model[2,0] - model[0,0]*i) / model[1,0])

plt.scatter(dX[:,0], dX[:,1], color = 'orange')
plt.plot(np.arange(0,1,0.04), plot_pred, color = 'green') 
plt.show()

#print(risks)
plt.figure(figsize=(16, 6))
plt.plot(np.arange(0,count), errs, color = 'orange')
plt.plot(np.arange(0,count), risks, color = 'green')