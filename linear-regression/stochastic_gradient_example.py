# -*- coding: utf-8 -*-
"""ML_Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16NjpX0GvSHGnbJBmGh03ijhrtpvphko8

Machine Learning Assignment 2
"""

!pip3 install torch torchvision torchaudio scipy==1.7.1

import numpy as np
from numpy.linalg import inv, pinv
import matplotlib.pyplot as plt
import scipy
import scipy.io
# Check Scipy version
print(scipy.__version__)

# load the mat files and dataset to the numpy array
mat_dataset = scipy.io.loadmat('/content/drive/MyDrive/ML_Homeworks/data3.mat')
dX = mat_dataset['data'][:,:2]
dY = mat_dataset['data'][:,2]
# Add Axis to match dimensions
dY = dY[..., np.newaxis]


# Concatenate array of ones on axis 1 to make room for bias.
dX = np.concatenate((dX, np.ones((dX.shape[0], 1))), axis = 1)
print(dX.shape, dY.shape)

"""
Perceptron loss
The perceptron loss is given as R(theta) = sum(y(x*theta)) / N
the gradient then simply is grad_R = sum(y*x) / N
In reality the perceptron loss is actually penalizing only the values of samples
y*f < 0. Which means we are finding the loss only for those values. Thus, while
finding the perceptron loss we only first check the above condition. The same
goes for the gradient descent update. Below loop while help understand the code.
"""

def preceptron_loss_and_gradient(x, y, learning_rate = 0.01, tolerance = 1e-3):
  """
  This function implements the stochastic gradient descent function on perceptron
  loss.
  """
  

  perceptron_loss = []
  binary_loss = []
  model = np.random.rand(3,1) # array with random values using random.rand
  model_old = np.random.rand(3,1)
  all_correct = False
  while not all_correct:
    R_perceptron = 0
    R_binary = 0
    misclassified = 0
    for i in range(dX.shape[0]):
      if dY[i] * np.dot(dX[i], model)[0] < 0:
        R_perceptron += dY[i] * np.dot(dX[i], model)[0]
        change = learning_rate*np.dot(dY[i][0], dX[i,]).reshape((3,1))
        model_old = model.copy()
        model = np.add(model, change)    
        misclassified += 1
        R_binary += 1
    binary_loss.append(-R_binary / dY.shape[0])
    if misclassified == 0:
      all_correct = True

    perceptron_loss.append(R_perceptron/dY.shape[0])


  return binary_loss, perceptron_loss, model


perceptron_loss = []
binary_loss = []
binary_loss, perceptron_loss, model = preceptron_loss_and_gradient(dX, dY, learning_rate=0.05)

plot_pred = []

for i in np.arange(0, 1, 0.04):
  plot_pred.append( (- model[2,0] - model[0,0]*i) / model[1,0])


plt.figure(figsize=(16, 6))
plt.scatter(dX[:,0], dX[:,1], color='blue')
plt.plot(np.arange(0,1,0.04), plot_pred, color = 'green') 
plt.show()


plt.plot(np.arange(0, len(perceptron_loss)), perceptron_loss, color = 'orange')
plt.plot(np.arange(0, len(binary_loss)), binary_loss, color = 'green')
plt.gca().invert_yaxis()